---
title: KAFKA (Producer,Consumer) ì‹¤ìŠµ
layout: single
categories: 
   - DataEngineering
author_profile: true
toc: true
toc_label: "ëª©ë¡"
toc_icon: "bars"
toc_sticky: true
date: 2023-03-15
---
## KAFKA (Producer,Consumer) ì‹¤ìŠµ

### Producer

intellijì—ì„œ JAVAë¥¼ í†µí•´ ìœ„ì—ì„œ KAFKA ì‰˜ì—ì„œ ê°„ë‹¨íˆ ì‹¤í–‰í•˜ì˜€ë˜ Producerë¥¼ êµ¬í˜„í•œë‹¤.

### 1. build.gradle

```java
plugins {
id 'java'
}
group 'de.kafka'
version '1.0-SNAPSHOT'
repositories {
mavenCentral()
}
dependencies {
testImplementation 'org.junit.jupiter:junit-jupiter-api:5.7.0'
testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.7.0'
implementation 'org.apache.kafka:kafka-clients:3.2.3'
implementation 'org.slf4j:slf4j-simple:1.7.30'
}
test {
useJUnitPlatform()
}
```

### 2. Main

```java
package de.kafka.basic.producer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.serialization.StringSerializer;
import java.util.Properties;
import java.util.Scanner;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;
public class Main {
	public static void main(String[] args) {
		Scanner scanner = new Scanner(System.in);
		Properties props = new Properties();
		// NODE1, NODE2, NODE3ì„ ë¸Œë¡œì»¤ ì„œë²„ IPë¡œ ë³€ê²½
		String BOOTSTRAP_SERVERS = "NODE1:9092,NODE2:9092,NODE3:9092";
		String TOPIC_NAME = "quickstart-events";
		props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
		props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.g
		etName()); // ì…ë ¥í•  ë©”ì‹œì§€ì˜ keyê°’ì˜ í˜•ì‹
		props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.clas
		s.getName()); // ì…ë ¥í•  ë©”ì„¸ì§€ì˜ í˜•ì‹
		Producer<String, String> producer = new KafkaProducer<>(props); // ë©”ì‹œì§€ì˜ í˜•ì‹ì— ë§ê²Œ genericsì„¤ì •
		while (true) {
				String key = null;
				System.out.println("Do you want to set a partition key? (y/n)");
				String isKeyRequired = scanner.nextLine();
				if (isKeyRequired.equals("y")) {
						System.out.println("Enter partition key: ");
						key = scanner.nextLine();
				}
				System.out.println("Enter string to send: ");
				String data = scanner.nextLine();
				ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC_NAME, k
				ey, data);
				Future<RecordMetadata> result = producer.send(record);
				producer.flush();
				try {
						RecordMetadata metadata = result.get();
						System.out.println("Message sent successfully to partition " + metadat
a.partition() + " with offset " + metadata.offset());
				} catch (InterruptedException | ExecutionException e) {
						e.printStackTrace();
				}
				System.out.println("Do you want to send another message? (y/n)");
				String isContinue = scanner.nextLine();
				if (isContinue.equals("n")) {
						break;
				}
		}
		producer.close();
	}
}
```

ë‹¤ìŒê³¼ ê°™ì´ JAVAë¡œ Producerë¥¼ êµ¬í˜„ì´ ê°€ëŠ¥í•˜ë‹¤.

recordë¥¼ ì „ì†¡í• ë•Œ ë¹„ë™ê¸°ì²˜ë¦¬ë¥¼ í•˜ì—¬, ì•„ë˜ì˜ ì˜ˆì™¸ì²˜ë¦¬ êµ¬ë¬¸ì„ ëŒ€ì²´í•  ìˆ˜ ìˆê²Œ êµ¬ì„±í•˜ì—¬ë„ ëœë‹¤.

```java
Future<RecordMetadata> result = producer.send(record,((metadata, exception) -> {
                if(exception!=null){
                    System.err.println("fail to record"+data);
                }
                else{
                    System.out.println("Message sent successfully to partition " + metadata.partition() + " with offset " + metadata.offset());
                }

                }
                )
            );
```

### Consumer

build.gradleì€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•œë‹¤.

### 1.Main

```java
package de.kafka.basic.consumer;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import java.util.Scanner;

public class Main {
    public static void main(String[] args) {
        Scanner scanner = new Scanner(System.in);
        Properties props = new Properties();

        String BOOTSTRAP_SERVERS = "13.125.34.64:9092,3.39.222.85:9092,43.201.86.33:9092";
        String TOPIC_NAME = "quickstart-events";
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "test01");
				//consumer group idë¡œ ê³„ì†ë‹¤ë¥´ê²Œ ì§€ì •í•´ì£¼ì–´ì•¼í•œë‹¤.
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
				// Kafkaì— ì´ˆê¸° ì˜¤í”„ì…‹ì´ ì—†ê±°ë‚˜ ì„œë²„ì— í˜„ì¬ ì˜¤í”„ì…‹ì´ ë” ì´ìƒ ì—†ëŠ” ê²½ìš° ìˆ˜í–‰í•  ì‘ì—…
				//"What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted): 
				//<ul><li>earliest: automatically reset the offset to the earliest offset<li>
				//latest: automatically reset the offset to the latest offset</li><li>
				//none: throw exception to the consumer if no previous offset is found for the consumer's group</li><li>anything else: throw exception to the consumer.</li></ul>";
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");
				// ìë™ ì»¤ë°‹ì´ ê¸°ë³¸ê°’ì´ì§€ë§Œ, ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ 0ë²ˆ íŒŒí‹°ì…˜ì€ commití•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤ë¼ë©´ falseí›„ ì•„ë˜ì—ì„œ consumer.commitSyncì„ ì´ìš©í•´ì•¼í•œë‹¤.

        Consumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList(TOPIC_NAME));
        try {
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                for (ConsumerRecord<String, String> record : records) {
                    System.out.println(
                            "Received message: (" + record.key() + ", " + record.value
                                    () + ") at " + "topic "
                                    + record.topic() + " partition " + record.partition() +
                                    "offset " + record.offset());
                }
            }
        } finally {
            consumer.close();
        }
    }
}
```

### 2. ì„¤ì •

**group.id** : consumerëŠ” ê°™ì€ group id ë‹¨ìœ„ë¡œ offset ê´€ë¦¬ë¥¼ í•œë‹¤. consumer ê°€ ì¢…ë£Œë˜ë”ë¼ë„ group id ë¥¼ ê¸°ì¤€ìœ¼ë¡œ commit ëœ offset ì •ë³´ëŠ” kafka ì— ìœ ì§€ë˜ê³ , ì´ì— ë”°ë¼ ê°™ì€ group idë¥¼ ê°€ì§„ consumerê°€ ë‹¤ìŒ consume ì‹œ ë°›ì•„ì˜¤ëŠ” ë°ì´í„°ëŠ” ê·¸ commitëœ offset ì´í›„ë¶€í„° ê°€ì ¸ì˜¨ë‹¤.

**auto.offset.reset** : í•´ë‹¹ consumer group id ì— ëŒ€í•œ offset ì •ë³´ê°€ ì—†ì„ ë•Œ, í˜„ì¬kafkaì˜ í† í”½ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì¤‘ì— ì–´ë””ë¶€í„° ê°€ì ¸ì˜¬ì§€.

1. **latest(default)**: ë§ˆì§€ë§‰ ë¶€í„° ê°€ì ¸ì˜¨ë‹¤. ì¦‰, consumer ê°€ ë¶™ì€ ì´í›„ë¶€í„° produceëœ ë°ì´í„°ë¶€í„° ê°€ì ¸ì˜¨ë‹¤.

2. **earliest**: í˜„ì¬ kafka ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„° ì¤‘ ê°€ì¥ ë¹ ë¥¸(ì˜¤ë˜ëœ) ê²ƒë¶€í„° ê°€ì ¸ì˜¨ë‹¤. ì´ë•Œ ì£¼ì˜í•  ì ì€ ìŒ“ì¸ ë°ì´í„° ì–‘ì— ë”°ë¼ì„œ ì´ˆë°˜ì— ë°ì´í„° consumeì–‘ì´ ë§ì•„ì§€ë¯€ë¡œ consumer ì— ë¶€ë‹´ì´ ë  ìˆ˜ ìˆë‹¤.

### 1. Auto Commit = False

ë§Œì¼ ìë™ ì»¤ë°‹ì„ í•˜ê²Œ ì„¤ì •í•˜ì§€ ì•Šê³ , íŠ¹ì • ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ë©”ë‰´ì–¼ ì»¤ë°‹ì„ í•´ì•¼í•œë‹¤ëŠ” ìƒí™©ì„ ê°€ì •í–ˆì„ë•Œ,

<aside>
ğŸ’¡ props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, â€œfalse");

</aside>

ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•´ì•¼í•œë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ìˆ˜ë§ì€ ì»¤ë°‹ìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆê³ , ì´ëŠ” ì‹œìŠ¤í…œì˜ ë¶€í•˜ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì£¼ì˜í•´ì•¼í•œë‹¤.

ë§Œì¼ ë™ì¼ Topicì— ëŒ€í•´ì„œ ê°€ì§€ëŠ” íŒŒí‹°ì…˜ì¤‘ì—ì„œ 0ë²ˆ íŒŒí‹°ì…˜ì„ ì»¤ë°‹ì„ í•˜ì§€ë§ê³ , ê³„ì† ìœ ì§€ë¥¼ í•´ì•¼í•œë‹¤ëŠ” ê°€ì •ì„ ë‘ê³  Consumerì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì

```java
package de.kafka.basic.consumer;
import java.time.Duration;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Properties;
import java.util.Scanner;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;

public class Main {
    public static void main(String[] args) {
        Scanner scanner = new Scanner(System.in);
        Properties props = new Properties();
// NODE1, NODE2, NODE3ì„ ë¸Œë¡œì»¤ ì„œë²„ IPë¡œ ë³€ê²½
        String BOOTSTRAP_SERVERS ="ec2-43-201-104-214.ap-northeast-2.compute.amazonaws.com:9092,ec2-3-36 -49-89.ap-northeast-2.compute.amazonaws.com:9092,ec2-3-38-153-68.ap-northeast-2.comput e.amazonaws.com:9092";
        String TOPIC_NAME = "quickstart-events";
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "test3");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
        Consumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList(TOPIC_NAME));
        try {
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));

                Map<TopicPartition, List<Long>> processedTopicPartitionOffsets = new HashMap<>();

                for (ConsumerRecord<String, String> record : records) {
                    System.out.println(
                            "Received message: (" + record.key() + ", " + record.value() + ") at "
                                    + "topic "+ record.topic() + " partition " + record.partition() + "offset "
                                    + record.offset());
                    if (record.partition() != 0) {
                        TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition());

                        if (processedTopicPartitionOffsets.containsKey(topicPartition)) {

                            List<Long> offsets = processedTopicPartitionOffsets.get(topicPartition);

                            offsets.add(record.offset());
                        }else {
                            List<Long> offsets = new ArrayList<>();
                            offsets.add(record.offset());
                            processedTopicPartitionOffsets.put(topicPartition, offsets);

                        }
                    }
                }
                Map<TopicPartition, OffsetAndMetadata> commitOffset = new HashMap();

                for (Entry<TopicPartition, List<Long>> entry : processedTopicPartition

                Offsets.entrySet()) {

                    List<Long> offsets = entry.getValue();
                    offsets.sort(Comparator.comparingLong(Long::longValue).reversed());

                    Long latestProcessedOffset = offsets.get(0);
                    System.out.println("latest processed offset of topic partition - "+ entry.getKey()
                            + ": " + latestProcessedOffset);
                    commitOffset.put(entry.getKey(), new OffsetAndMetadata(latestProcessedOffset + 1));
                }
                consumer.commitSync(commitOffset);
            }
        } finally {
            consumer.close();
        }
    }
}

```

```java
# ì½”ë“œ ì¤‘ í•µì‹¬ì ì¸ ë¶€ë¶„
props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false");
# ì»¨ìŠˆë¨¸ì˜ auto commit ì„¤ì •ì„ falseë¡œ ì§€ì •í•˜ì˜€ë‹¤.

Map<TopicPartition, List<Long>> processedTopicPartitionOffsets = new HashMap<>();
# HashMapìœ¼ë¡œ Genericsë¥¼ 1.í† í”½íŒŒí‹°ì…˜ 2.ì˜¤í”„ì…‹ì„ ë‹´ëŠ” Longí˜• ë¦¬ìŠ¤íŠ¸ìœ¼ë¡œ êµ¬ì„±í•˜ì˜€ë‹¤.
# ì´ëŠ” TopicPartionë³„ë¡œ ì˜¤í”„ì…‹ì •ë³´ë¥¼ ê¸°ë¡í•  ìˆ˜ ìˆë‹¤.

if (record.partition() != 0) {
# recordëŠ” ë£¨í”„ë¥¼ ëŒë©´ì„œ conusmerê°€ êµ¬ë…í•˜ëŠ” ê°’ì´ë‹¤. (ê¸°ë³¸ì ìœ¼ë¡œ í† í”½íŒŒí‹°ì…˜ì •ë³´ì™€ ì˜¤í”„ì…‹ì •ë³´ë¥¼ ì…ë ¥ì„ ë°›ê¸°ë•Œë¬¸ì— í•„ë“œë¡œ ì¡´ì¬í•œë‹¤.)
# ìš”êµ¬ì‚¬í•­ì¸ íŒŒí‹°ì…˜ì´ 0ë²ˆíŒŒí‹°ì…˜ì´ ì•„ë‹Œê²½ìš°ì—ë§Œ,
            TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition());
						# ìœ„ì—ì„œ ì„ ì–¸í•œ processedTopicPartionOffsetsì˜ Genericì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´
						# TopicPartionì´ í‚¤ê°’ìœ¼ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ, recordê°€ ì§€ë‹ˆê³ ìˆëŠ” í† í”½ê³¼ íŒŒí‹°ì…˜ì„ ì´ìš©í•˜ì—¬,
						# ê°ì²´ë¥¼ ì„ ì–¸í•´ì¤€ê²ƒì´ë‹¤.						
            if (processedTopicPartitionOffsets.containsKey(topicPartition)) {
						# ë§Œì¼ ì´ë¯¸ í•´ì‹œë§µì— ì¡´ì¬í•œë‹¤ë©´, (ì´ê±´ 0ì´ì•„ë‹Œ íŒŒí‹°ì…˜ì´ë‹¤.)

	          List<Long> offsets = processedTopicPartitionOffsets.get(topicPartition);
						# ì´ë¯¸ ì¡´ì¬í•˜ê¸°ì—, hashmapì—ì„œ topicpartionì— í•´ë‹¹í•˜ëŠ” offsetsë¥¼ getìœ¼ë¡œ ë¶ˆëŸ¬ì™€
						
            offsets.add(record.offset());
						# ë’¤ì— addì‹œì¼œì£¼ë©´ëœë‹¤.
            }else {
						# 0ì´ì•„ë‹Œ íŒŒí‹°ì…˜ì¸ë° ì¡´ì¬ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´,
                      List<Long> offsets = new ArrayList<>();
											# ìƒˆë¡œìš´ offsetë¦¬ìŠ¤íŠ¸ë¥¼ ì„ ì–¸í•´ì¤˜ì•¼í•˜ë©°,
                      offsets.add(record.offset());
											# í˜„ì¬ë“¤ì–´ì˜¨ ë ˆì½”ë“œì˜ offsetì„ ê¸°ë¡í•˜ê³ ,
                      processedTopicPartitionOffsets.put(topicPartition, offsets);
											# í•´ì‹œë§µì— putí•´ì¤€ë‹¤.

                   }

```

```java
Map<TopicPartition, OffsetAndMetadata> commitOffset = new HashMap();
# consumerì˜ commitSyncì˜ íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ê°€ëŠ” <TopicPartition, OffsetAndMetadata>íƒ€ì…ìœ¼ë¡œ HashMapì„ ìƒì„±í•œë‹¤.

      for (Entry<TopicPartition, List<Long>> entry : processedTopicPartitionOffsets.entrySet()) {
			# hashMapì„ iterí•˜ê¸° ìœ„í•œ Entryì…‹ìœ¼ë¡œ ìºìŠ¤íŒ…í•œë‹¤.
               List<Long> offsets = entry.getValue();
							 # ì˜¤í”„ì…‹ì •ë³´ë¥¼ listë¡œ ë°›ì•„ì˜¤ê³ ,
               offsets.sort(Comparator.comparingLong(Long::longValue).reversed());
							# ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•œë‹¤. ( commitSyncëŠ” ì˜¤í”„ì…‹ì˜ ë§ˆì§€ë§‰ +1ì„ ìš”êµ¬í•œë‹¤.)

               Long latestProcessedOffset = offsets.get(0);
							# 0ë²ˆì§€(ê°€ì¥í° ì˜¤í”„ì…‹ì •ë³´)ë¥¼ ì €ì¥í•˜ê³ 
               System.out.println("latest processed offset of topic partition - "+ entry.getKey()
                            + ": " + latestProcessedOffset);
               commitOffset.put(entry.getKey(), new OffsetAndMetadata(latestProcessedOffset + 1));
							# keyê°’ê³¼ ê°€ì¥í° ì˜¤í”„ì…‹ì •ë³´+1ë¥¼ commitOffsetì— ë„£ê³ ,
           }
           consumer.commitSync(commitOffset);
					# ì»¤ë°‹í•œë‹¤.
```