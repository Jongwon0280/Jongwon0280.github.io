---
layout: single
title:  Transformers Architecture with TensorFlow
categories:
  - ML
author_profile: true
toc: true
toc_label: "ëª©ë¡"
toc_icon: "bars"
toc_sticky: true
date: 2023-12-11
---

## Transformers Architecture with TensorFlow

> **ëª©ì **
> 

íŠ¸ëœìŠ¤í¬ë¨¸ Seq2Seq ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•˜ê³ , ì§ì ‘ tensorflowë¡œ êµ¬í˜„í•˜ëŠ” ëª©ì ì´ë‹¤.

> **í•„ìš”ì„±**
> 

Seq2SeqëŠ” Transformer ì´ì „ì—, Bi-LSTMê¸°ë°˜ ì¸ì½”ë”ì™€ LSTMì˜ ì¸ì½”ë”ì—ì„œ attentionë¥¼ ì ìš©í•˜ì—¬, í•´ê²°í•˜ì˜€ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²• ì—­ì‹œ, ê²Œì´íŠ¸ê¸°ë°˜ RNNêµ¬ì¡°ë¡œ ì¥ê¸°ì˜ì¡´ì„±ì„ í•´ê²°í•˜ëŠ” êµ¬ì¡°ì§€ë§Œ ì™„ì „íˆ í•´ê²°í•˜ì§€ ëª»í•˜ë©°, ê¸°ìš¸ê¸° ì†Œë©¸ë¬¸ì œì˜ ì—¬ì§€ê°€ ìˆë‹¤. ê·¸ë˜ì„œ ë‚˜ì˜¨ TransformerëŠ” LSTMì´ë¼ëŠ” ëª¨ë“ˆ ìì²´ë¥¼ ì—†ì• ê³ , ì´ë¥¼ Self-Attentionì´ë¼ëŠ” ê¸°ë²•, ì¦‰ Attentionë§Œìœ¼ë¡œ ì¸ì½”ë”©í•œë‹¤.

> **êµ¬í˜„**
> 

- **ë¼ì´ë¸ŒëŸ¬ë¦¬**
    
    ```python
    # Tensorflow í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•œë‹¤.
    
    import tensorflow as tf
    import time
    import numpy as np
    import matplotlib.pyplot as plt
    
    from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization
    from transformers import DistilBertTokenizerFast #, TFDistilBertModel
    from transformers import TFDistilBertForTokenClassification # pytorchì—ì„œëŠ” BertForSequenceClassification
    ```
    
- ****Positional Encoding****
    
    RNNê¸°ë°˜ì˜ LSTMì—ì„œëŠ” ìˆœì°¨ì ì¸ ëª¨ë¸ì˜ íŠ¹ì„±ìƒ ìœ„ì¹˜ì •ë³´ë¥¼ ë”°ë¡œ ì¤„ í•„ìš”ê°€ ì—†ì§€ë§Œ, íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œëŠ” ëª¨ë“  ì‹œí€¸ìŠ¤ë¥¼ ë™ë“±í•˜ê²Œ ì…ë ¥í•´ì£¼ê¸°ë•Œë¬¸ì—, ìˆœì„œì •ë³´ë¥¼ ìƒëŠ”ë‹¤. ì´ë¥¼ ìœ„í•´ Pos Encodingì´ í•„ìš”í•˜ë‹¤.
    
    ![Untitled](https://github.com/Jongwon0280/Jongwon0280.github.io/assets/56438131/e91a9814-a413-4285-8f81-61b9f4222ff1)
    
    dëŠ” ë‹¨ì–´ì„ë² ë”©ì˜ ì°¨ì›ì„ ëœ»í•˜ê³ , posëŠ” ë‹¨ì–´ì˜ ìœ„ì¹˜, i = k//2
    
    cosê³¼ sinì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ë‹¨ìˆœ í•˜ë“œì½”ë”©ì„ í†µí•´ ìœ„ì¹˜ì¸ì½”ë”©ì„ í•˜ë©´ ì˜ë¯¸ë¡ ì ì˜ë¯¸ê°€ ì™œê³¡ ë˜ê¸°ì—, ì™œê³¡ë˜ì§€ì•Šê³ , ê°’ì´ ì‘ì€ cos,sinì„ ì´ìš©í•œë‹¤.
    
    ![Untitled 1](https://github.com/Jongwon0280/Jongwon0280.github.io/assets/56438131/8797193a-1b8b-47c9-9281-c8be65c61252)
    
    - **get angles**
        
        ```python
        def get_angles(pos, k, d):
            """
            Get the angles for the positional encoding
            
            Arguments:
                pos -- Column vector containing the positions [[0], [1], ...,[N-1]]
                k --   Row vector containing the dimension span [[0, 1, 2, ..., d-1]]
                d(integer) -- Encoding size
            
            Returns:
                angles -- (pos, d) numpy array 
            """
            
            
            # Get i from dimension span k
            i = k//2
            # ì—¬ê¸°ì„œ ì‹œí€¸ìŠ¤ ìœ„ì¹˜ë¥¼ ë‹´ëŠ” pos ì—´ë²¡í„°ê°€ ë¸Œë¡œë“œìºìŠ¤íŒ… ë˜ì–´
            # ê° ì‹œí€¸ìŠ¤ ìœ„ì¹˜ì— ëŒ€í•´ iì¸ë±ìŠ¤ì— ëŒ€í•´ ì—°ì‚°í•œë‹¤.
            angles = pos / (10000**(2*i/d))
          
            
            return angles
        ```
        
    - **Pos Encoding**
        
        ```python
        
        def positional_encoding(positions, d):
            """
            Precomputes a matrix with all the positional encodings 
            
            Arguments:
                positions (int) -- Maximum number of positions to be encoded 
                d (int) -- Encoding size 
            
            Returns:
                pos_encoding -- (1, position, d_model) A matrix with the positional encodings
            """
            
            angle_rads = get_angles(np.arange(positions)[:, np.newaxis], # ì—´ë²¡í„°
                                    np.arange(d)[np.newaxis, :], # í–‰ë²¡í„°
                                    d)
          
            # ì§ìˆ˜ëŠ” sin
            angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
          
            # í™€ìˆ˜ëŠ” cos
            angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
            
            
            pos_encoding = angle_rads[np.newaxis, ...]
            
            return tf.cast(pos_encoding, dtype=tf.float32)
        
        pos_encoding = positional_encoding(50, 512)
        ```
        
        ![Untitled 2](https://github.com/Jongwon0280/Jongwon0280.github.io/assets/56438131/5ca50347-4d0f-47b5-b6af-a0053d6131bf)
        
- **ë§ˆìŠ¤í‚¹**
    
    
    - **íŒ¨ë”©ë§ˆìŠ¤í¬**
        
        
        ì…ë ¥ ì‹œí€¸ìŠ¤ì˜ ìµœëŒ€ê¸¸ì´ëŠ” ì •í•´ì ¸ìˆì§€ë§Œ, ê°ê° ë‹¤ë¥¸ ì‹œí€¸ìŠ¤ê¸¸ì´ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì—, ê¸¸ì´ê°€ ì§§ì€ í† í°ë“¤ì€ íŒ¨ë”©ì´ í•„ìš”í•˜ë‹¤.
        
        <aside>
        ğŸ—½
        
        ```
        [["Do", "you", "know", "when", "Jane", "is", "going", "to", "visit", "Africa"],
         ["Jane", "visits", "Africa", "in", "September" ],
         ["Exciting", "!"]
        ]
        
        ```
        
        ```
        [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],
         [ 56, 1285, 15, 181, 545],
         [ 87, 600]
        ]
        ```
        
        ```
        [[ 71, 121, 4, 56, 99],
         [ 2344, 345, 1284, 15, 0],
         [ 56, 1285, 15, 181, 545],
         [ 87, 600, 0, 0, 0],
        ]
        ```
        
        </aside>
        
        ```python
        def create_padding_mask(decoder_token_ids):
            """
            Creates a matrix mask for the padding cells
            
            Arguments:
                decoder_token_ids -- (n, m) matrix
            
            Returns:
                mask -- (n, 1, m) binary tensor
            """    
            seq = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)
          
            # add extra dimensions to add the padding
            # to the attention logits. 
            # this will allow for broadcasting later when comparing sequences
            return seq[:, tf.newaxis, :]
        ```
        
        ì£¼ì˜í• ì ì€ 0ì—ëŒ€í•œ ì†Œí”„íŠ¸ë§¥ìŠ¤ê°€ ì˜í–¥ë ¥ì„ ë°œíœ˜í•˜ì§€ ì•Šê²Œ í•˜ê¸°ìœ„í•´ì„œëŠ”, (1-x) * -1.0e9)í•´ì£¼ë©´ 0ìœ¼ë¡œ ìˆ˜ë ´í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤.
        
        ```python
        print(tf.keras.activations.softmax(x))
        print(tf.keras.activations.softmax(x + (1 - create_padding_mask(x)) * -1.0e9))
        
        """
        tf.Tensor(
        [[7.2876644e-01 2.6809821e-01 6.6454901e-04 6.6454901e-04 1.8064314e-03]
         [8.4437378e-02 2.2952460e-01 6.2391251e-01 3.1062774e-02 3.1062774e-02]
         [4.8541026e-03 4.8541026e-03 4.8541026e-03 2.6502505e-01 7.2041273e-01]], shape=(3, 5), dtype=float32)
        tf.Tensor(
        [[[7.2973627e-01 2.6845497e-01 0.0000000e+00 0.0000000e+00 1.8088354e-03]
          [2.4472848e-01 6.6524094e-01 0.0000000e+00 0.0000000e+00 9.0030573e-02]
          [6.6483547e-03 6.6483547e-03 0.0000000e+00 0.0000000e+00 9.8670328e-01]]
        
         [[7.3057163e-01 2.6876229e-01 6.6619506e-04 0.0000000e+00 0.0000000e+00]
          [9.0030573e-02 2.4472848e-01 6.6524094e-01 0.0000000e+00 0.0000000e+00]
          [3.3333334e-01 3.3333334e-01 3.3333334e-01 0.0000000e+00 0.0000000e+00]]
        
         [[0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01 7.3105860e-01]
          [0.0000000e+00 0.0000000e+00 0.0000000e+00 5.0000000e-01 5.0000000e-01]
          [0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6894143e-01 7.3105860e-01]]], shape=(3, 3, 5), dtype=float32)
        """
        ```
        
    
    - **Look-Ahead Mask**
        
        ë””ì½”ë” ë‹¨ê³„ì—ì„œ masked-MHAë¥¼ ì‚¬ìš©í• ë•Œ í•„ìš”í•œ ë§ˆìŠ¤í¬ì´ë‹¤.
        
        ```python
        def create_look_ahead_mask(sequence_length):
            """
            Returns a lower triangular matrix filled with ones
            
            Arguments:
                sequence_length -- matrix size
            
            Returns:
                mask -- (size, size) tensor
            """
            mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)
            return mask
        ```
        
    
- **Self-Attention**
    
    
    ![Untitled 3](https://github.com/Jongwon0280/Jongwon0280.github.io/assets/56438131/b804d620-1eaa-4c36-b360-eb1c0ae2df7d)
    
    ìµœëŒ€ ì‹œí€¸ìŠ¤ ê¸¸ì´ì•ˆì˜ ëª¨ë“  í† í°ë“¤ì„ Wq, Wk, Wv (ê³µìœ ë˜ëŠ” íŒŒë¼ë¯¸í„°)ë¥¼ í†µí•´ ê³ ì •ëœ ì„ë² ë”© ì°¨ì› q, k, ví˜•íƒœë¡œ ë³€í™˜í•œë‹¤. ì´í›„ ê° ì‹œí€¸ìŠ¤ì˜ queryëŠ” ëª¨ë“  ì‹œí€¸ìŠ¤ keyì™€ì˜ ë‚´ì ì„ í†µí•´ ê°ì‹œí€¸ìŠ¤ë³„ë¡œ len(k)ì˜ ê°’ì„ ì–»ì–´, (seq_q, seq_k) í–‰ë ¬ì„ ì–»ê³  ì´ë¥¼ softmaxì— í†µê³¼ì‹œì¼œ í™•ë¥ ë¶„í¬ë¡œ ë³€í™˜í•œë‹¤. ì´í›„ ê° ì‹œí€¸ìŠ¤ë³„ë¡œ vë¥¼ ë‚´ì í•˜ì—¬ ê° ì‹œí€¸ìŠ¤ë³„ë¡œ ë”í•œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ (ì‹œí€¸ìŠ¤ê¸¸ì´, ì„ë² ë”©ì°¨ì›)ì˜ ê²°ê³¼ê°’ì´ ë‚˜ì˜¨ë‹¤.
    
    ![Untitled 4](https://github.com/Jongwon0280/Jongwon0280.github.io/assets/56438131/80744435-d53b-4998-9fe0-0ba870283be5)
    
    ```python
    # UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)
    # GRADED FUNCTION scaled_dot_product_attention
    def scaled_dot_product_attention(q, k, v, mask):
        """
        Calculate the attention weights.
          q, k, v must have matching leading dimensions.
          k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.
          The mask has different shapes depending on its type(padding or look ahead) 
          but it must be broadcastable for addition.
    
        Arguments:
            q -- query shape == (..., seq_len_q, depth)
            k -- key shape == (..., seq_len_k, depth)
            v -- value shape == (..., seq_len_v, depth_v)
            mask: Float tensor with shape broadcastable 
                  to (..., seq_len_q, seq_len_k). Defaults to None.
    
        Returns:
            output -- attention_weights
        """
       
        
        matmul_qk = tf.matmul(q,k,transpose_b=True)  # (..., seq_len_q, seq_len_k)
    
        # scale matmul_qk
        
        dk = float(q.shape[-1])
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
    
        # softmaxì—ì„œ íŒ¨ë”©ë§ˆìŠ¤í¬ 0ì— í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì˜ ì˜í–¥ë ¥ì„ ì—†ì• ê¸° ìœ„í•¨ì´ë‹¤.
        if mask is not None: 
            scaled_attention_logits += ((1. - mask) * -1e9) 
    
        # ê° ì‹œí€¸ìŠ¤ì˜ ì¿¼ë¦¬ì— ëŒ€í•´ ëª¨ë“  ì‹œí€¸ìŠ¤ì™€ì˜ í‚¤ì™€ì˜ ë‚´ì ì— ëŒ€í•´ì„œ softmaxë¥¼ ì·¨í•´ í™•ë¥  ë¶„í¬ë¡œ ë°”ê¾¼ë‹¤.
      
        attention_weights = tf.keras.activations.softmax(scaled_attention_logits,axis=-1)  # (..., seq_len_q, seq_len_k)
    
        output = tf.matmul(attention_weights, v)  
    		# ê° ì‹œí€¸ìŠ¤ì˜ queryì—ëŒ€í•œ ê° ì‹œí€¸ìŠ¤ì˜ keyì— ëŒ€í•œ attention scoreë¥¼ ê° ì‹œí€¸ìŠ¤ì— ëŒ€í•´ êµ¬í•  ë•Œë§ˆë‹¤ ëª¨ë“  ì‹œí€¸ìŠ¤-
    		#ì— valueë¥¼ ë‚´ì í•´ì¤˜ì•¼ í•˜ë¯€ë¡œ ê°€ì¤‘í•©ì„ ìˆ˜í–‰í•œë‹¤.
        
       
    
        return output, attention_weights
    ```
    
- **Encoder**
    
    ![Untitled 5](https://github.com/Jongwon0280/Jongwon0280.github.io/assets/56438131/01b4092e-1938-43f0-9b7a-2b3940f9b3e4)
    
    ë©€í‹°í—¤ë“œ ì–´í…ì…˜ì´ë€, ì…€í”„ ì–´í…ì…˜ì„ headì˜ ìˆ˜ë§Œí¼ ìˆ˜í–‰í•˜ì—¬, concatí•˜ê³ , ì´ë¥¼ íŠ¹ì •ì°¨ì›ìœ¼ë¡œ ì„ë² ë”©ì‹œí‚¨ë‹¤. ê° ì…€í”„ì–´í…ì…˜ì—ì„œëŠ” ì„œë¡œ ë‹¤ë¥¸ ë¬¸ë§¥ì˜ íŠ¹ì§•ì„ ìº¡ì²˜í•  ìˆ˜ ìˆë‹¤.
    
    íŠ¹ì •ì°¨ì›ìœ¼ë¡œ ì„ë² ë”© ì‹œí‚¨ ë²¡í„°ì— skip-connectionì„ ë”í•´ì£¼ê³ , Layer ì •ê·œí™”ë¥¼ ìˆ˜í–‰ì‹œí‚¨ í›„ FFNNì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê³  ë˜‘ê°™ì´ skip-connectionê³¼ Layer ì •ê·œí™”ë¥¼ ì§„í–‰ì‹œí‚¨ë‹¤.
    
    ì´ê³¼ì •ì€ Encoder Blockì˜ 1íšŒ ìˆ˜í–‰ì‹œ í•˜ëŠ” ì¼ì´ê³ , ì´ë¥¼ kë²ˆìˆ˜í–‰í•œë‹¤.
    
     ë‹¤ìŒ ì½”ë“œëŠ” ì¸ì½”ë” ë¸”ë¡ì˜ FFNNì„ ì •ì˜í•œê²ƒì´ë‹¤.
    
    ```python
    def FullyConnected(embedding_dim, fully_connected_dim):
        return tf.keras.Sequential([
            tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)
            tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, embedding_dim)
        ])
    ```
    
    ì¸ì½”ë” ë ˆì´ì–´ë¥¼ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.
    
    ```python
    
    class EncoderLayer(tf.keras.layers.Layer):
       
        def __init__(self, embedding_dim, num_heads, fully_connected_dim,
                     dropout_rate=0.1, layernorm_eps=1e-6):
            super(EncoderLayer, self).__init__()
    
            self.mha = MultiHeadAttention(num_heads=num_heads,
                                          key_dim=embedding_dim,
                                          dropout=dropout_rate)
    
            self.ffn = FullyConnected(embedding_dim=embedding_dim,
                                      fully_connected_dim=fully_connected_dim)
    
            self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)
            self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)
    
            self.dropout_ffn = Dropout(dropout_rate)
        
        def call(self, x, training, mask):
            """
            
            
            Arguments:
                x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)
                training -- Boolean, set to true to activate
                            the training mode for dropout layers
                mask -- Boolean mask to ensure that the padding is not 
                        treated as part of the input
            Returns:
                encoder_layer_out -- Tensor of shape (batch_size, input_seq_len, embedding_dim)
            """
            
           
            # Dropout is added by Keras automatically if the dropout parameter is non-zero during training
            self_mha_output = self.mha(x,x,x,mask) # Wq, Wk, Wvì— ì—°ì‚°í•  x, x, xì™€, íŒ¨ë”© ë§ˆìŠ¤í¬ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤€ë‹¤.
    				# Self attention (batch_size, input_seq_len, embedding_dim)
            
            # skip connection
     
            skip_x_attention =  self.layernorm1(self_mha_output + x) # (batch_size, input_seq_len, embedding_dim)
    
            
            ffn_output = self.ffn(skip_x_attention)  # (batch_size, input_seq_len, embedding_dim)
            
            
            ffn_output = self.dropout_ffn(ffn_output,training=training)
            
       
            encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)  # (batch_size, input_seq_len, embedding_dim)
            
            
            return encoder_layer_out
    ```
    
- **Full Enocder**
    
    Encoder Layerë¥¼ kë²ˆ ìˆ˜í–‰í•œë‹¤. ì´ë¥¼ ë°˜ì˜í•œ Encoder Classë¥¼ ì •ì˜í•˜ì.
    
    ```python
    
    class Encoder(tf.keras.layers.Layer):
    	  """
    		Encoder layerë¥¼ ìŒ“ëŠ”ë‹¤.
    		"""
        def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,
                   maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):
            super(Encoder, self).__init__()
    
            self.embedding_dim = embedding_dim
            self.num_layers = num_layers
    
            self.embedding = Embedding(input_vocab_size, self.embedding_dim)
            self.pos_encoding = positional_encoding(maximum_position_encoding, 
                                                    self.embedding_dim)
    
            self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,
                                            num_heads=num_heads,
                                            fully_connected_dim=fully_connected_dim,
                                            dropout_rate=dropout_rate,
                                            layernorm_eps=layernorm_eps) 
                               for _ in range(self.num_layers)]
    
            self.dropout = Dropout(dropout_rate)
            
        def call(self, x, training, mask):
            """
            
            
            Arguments:
                x -- Tensor of shape (batch_size, input_seq_len)
                training -- Boolean, set to true to activate
                            the training mode for dropout layers
                mask -- Boolean mask to ensure that the padding is not 
                        treated as part of the input
            Returns:
                out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)
            """
            seq_len = tf.shape(x)[1]
            
            
            # Pass input through the Embedding layer
            x = self.embedding(x)  # (batch_size, input_seq_len, embedding_dim)
            # Scale embedding by multiplying it by the square root of the embedding dimension
            x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))
            # ì„ë² ë”©ì˜ ì œê³±ê·¼ì„ ê³±í•˜ëŠ” ì´ìœ ëŠ” (Q K^T) / (root(dk))ë¡œ ë‚˜ëˆ„ì–´, ìŠ¤ì¼€ì¼ë§ í•´ì£¼ê¸° ìœ„í•¨ì´ë‹¤.
            x += self.pos_encoding [:, :seq_len, :]
            # ì´ˆê¸°í™” ë‹¨ê³„ì—ì„œ ê³„ì‚°í•œ, (1, input_seq_len , embedding dims)ë¥¼ ë”í•´ì¤€ë‹¤.
            # use `training=training`
            x = self.dropout(x,training=training)
            # ë ˆì´ì–´ì˜ ìˆ˜ ë§Œí¼ encoder layerë¥¼ ìˆ˜í–‰í•œë‹¤. ìœ ë™ì ìœ¼ë¡œ ì¡°ì ˆ ê°€ëŠ¥í•˜ë‹¤. 
            for i in range(self.num_layers):
                x = self.enc_layers[i](x, training, mask) # ì²« ì¸ì½”ë”ì—ëŠ” ì…ë ¥ìœ¼ë¡œ x
    						# ì´í›„ì—ëŠ” encoder layerì˜ ì¶œë ¥ì„ xë¡œ ì…ë ¥í•˜ê¸° ìœ„í•¨ì´ë‹¤.
            
    
            return x  # (batch_size, input_seq_len, embedding_dim)
    ```
    
- **Decoder**
    
    
    ![Untitled 6](https://github.com/Jongwon0280/Jongwon0280.github.io/assets/56438131/5bbef429-44a8-4fbc-8ae7-de84581c3cfe)
    
    ë””ì½”ë”ëŠ” ì¸ì½”ë”ì˜ ì¶œë ¥ì„ í™œìš©í•˜ì—¬ seq2seqë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ êµ¬ì¡°ì´ë‹¤.
    
    ì¸ì½”ë”ì™€ ë§ˆì°¬ê°€ì§€ì˜ êµ¬ì¡°ì´ì§€ë§Œ, look-aheadë§ˆìŠ¤í¬ë¥¼ í™œìš©í•œ masked-multi head attentionê³¼, enc_outputì˜ key,valueì™€ masked-multi head attentionì˜ queryë¥¼ í™œìš©í•˜ì—¬ multi-head-attentionì„ ìˆ˜í–‰í•˜ê³ , ì‹œí€¸ìŠ¤ í•œê°œì”© ì˜ˆì¸¡í•˜ê³ , ì˜ˆì¸¡í•œ ì‹œí€¸ìŠ¤ì˜ ê°’ì„ decoderì˜ ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ í™œìš©í•œë‹¤. 
    
    ë‹¤ìŒì€ ë””ì½”ë” ë ˆì´ì–´ì— ëŒ€í•œ êµ¬í˜„ì´ë‹¤.
    
    ```python
    
    class DecoderLayer(tf.keras.layers.Layer):
       
        def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):
            super(DecoderLayer, self).__init__()
    
            self.mha1 = MultiHeadAttention(num_heads=num_heads,
                                          key_dim=embedding_dim,
                                          dropout=dropout_rate)
    
            self.mha2 = MultiHeadAttention(num_heads=num_heads,
                                          key_dim=embedding_dim,
                                          dropout=dropout_rate)
    
            self.ffn = FullyConnected(embedding_dim=embedding_dim,
                                      fully_connected_dim=fully_connected_dim)
    
            self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)
            self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)
            self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)
    
            self.dropout_ffn = Dropout(dropout_rate)
        
        def call(self, x, enc_output, training, look_ahead_mask, padding_mask):
            """
            
            
            Arguments:
                x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
                enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)
                training -- Boolean, set to true to activate
                            the training mode for dropout layers
                look_ahead_mask -- Boolean mask for the target_input
                padding_mask -- Boolean mask for the second multihead attention layer
            Returns:
                out3 -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
                attn_weights_block1 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)
                attn_weights_block2 -- Tensor of shape(batch_size, num_heads, target_seq_len, input_seq_len)
            """
            
            
            # enc_output.shape == (batch_size, input_seq_len, embedding_dim)
            
            # BLOCK 1
          
    				# look-ahead maskë¥¼ í†µí•´ maskingëœ ë§ˆìŠ¤í¬ë¥¼ ë„£ì–´ì¤€ë‹¤. Q,K,Vë¡œ ê³„ì‚°ë  x,x,xëŠ” ì¸ì½”ë”ì™€ ë™ì¼í•˜ë‹¤.
            mult_attn_out1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask, return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)
           
            Q1 = self.layernorm1(x+mult_attn_out1)
    
            # BLOCK 2
            
    				# Qì—ëŠ” ì´ì „ ë””ì½”ë” Masked-MHAì˜ ì¶œë ¥ì„, K,VëŠ” ì‚¬ì „ì— ê³„ì‚°í•œ Encoderì˜ outputì„ ì‚¬ìš©í•˜ê³ , íŒ¨ë”©ë§ˆìŠ¤í¬ë¥¼ ë„£ì–´ì¤€ë‹¤.
            mult_attn_out2, attn_weights_block2 = self.mha2(Q1,enc_output, enc_output, padding_mask, return_attention_scores=True)  # (batch_size, target_seq_len, embedding_dim)
            
            
            mult_attn_out2 = self.layernorm2(mult_attn_out2 + Q1)  # (batch_size, target_seq_len, embedding_dim)
                    
            #BLOCK 3
            # pass the output of the second block through a ffn
            ffn_output = self.ffn(mult_attn_out2)  # (batch_size, target_seq_len, embedding_dim)
            
           
            ffn_output = self.dropout_ffn(ffn_output)
            
           
            out3 = self.layernorm3(ffn_output +mult_attn_out2 )  # (batch_size, target_seq_len, embedding_dim)
            
    
            return out3, attn_weights_block1, attn_weights_block2
    ```
    
- **Full Decoder**
    
    ì´ì   ì „ì²´ ë””ì½”ë” êµ¬ì¡°ì—ëŒ€í•œ í´ë˜ìŠ¤ êµ¬í˜„ì´ë‹¤.
    
    ```python
    
    class Decoder(tf.keras.layers.Layer):
    
        def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,
                   maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):
            super(Decoder, self).__init__()
    
            self.embedding_dim = embedding_dim
            self.num_layers = num_layers
    
            self.embedding = Embedding(target_vocab_size, self.embedding_dim)
            self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)
    
            self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,
                                            num_heads=num_heads,
                                            fully_connected_dim=fully_connected_dim,
                                            dropout_rate=dropout_rate,
                                            layernorm_eps=layernorm_eps) 
                               for _ in range(self.num_layers)]
            self.dropout = Dropout(dropout_rate)
        
        def call(self, x, enc_output, training, 
               look_ahead_mask, padding_mask):
            """
            
            
            Arguments:
                x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
                enc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)
                training -- Boolean, set to true to activate
                            the training mode for dropout layers
                look_ahead_mask -- Boolean mask for the target_input
                padding_mask -- Boolean mask for the second multihead attention layer
            Returns:
                x -- Tensor of shape (batch_size, target_seq_len, embedding_dim)
                attention_weights - Dictionary of tensors containing all the attention weights
                                    each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)
            """
    
            seq_len = tf.shape(x)[1]
            attention_weights = {}
            
            
            # ì›Œë“œ ì„ë² ë”©ì„ ìƒì„±í•œë‹¤.
            x = self.embedding(x)  # (batch_size, target_seq_len, embedding_dim)
            
            # ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ root( ì„ë² ë”©ì°¨ì›)ìœ¼ë¡œ ê³±í•´ì¤€ë‹¤.
            x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))
            
            # ì´ˆê¸°í™” ë‹¨ê³„ì—ì„œ ê³„ì‚°í•œ pos encodingì„ ë”í•´ì¤˜, ìœ„ì¹˜ ì •ë³´ë¥¼ ì‚´ë¦°ë‹¤.
            x += self.pos_encoding [:, :seq_len, :]
    
            x = self.dropout(x,training=training)
    
            
            for i in range(self.num_layers):
                
                x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)
    
                
                attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = block1
                attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = block2
            
            
           
            return x, attention_weights
    ```