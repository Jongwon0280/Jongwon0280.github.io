# Elastic 분석기
title: Elastic 분석기
subtitle: 
categories: Elastic
tags: cloud Elastic[D[D[D[D[D[D[D[D[D[D[D[C[C[C[C[C[C[C[C[C[C[C[C[C[C
date: 2023-01-23 23:33:38 +0000
last_modified_at: 2023-01-23 23:33:39 +0000
---

### Setting

처음 인덱스를 정의할 때 샤드 수등을 정의한다.

### Mapping

각 필드별로 데이터를 저장하는 스키마 명세, 인덱스, 타입별로 구분된다.

동적매핑을 지원한다.

자동생성된 매핑의 텍스트 필드는 깁곤적으로 스탠다드 애널라이저이다.

### Data Type

문자열 → text, keyword

지리정보 → geo_point, geo_shape (키바나 지도에서 나타낼 수 있다.)

**text → 입력된 문자를 텀 단위로 쪼개어 역 색인 구조를 만든다.**

**보통 풀 텍스트 검색에 사용할 문자열 필드들을 text타입으로 지정한다.**

**token→ 하나의 토큰으로 저장한다.**

********object → JSON형태로 이루어져있기 때문에 여러필드를 가진 object 타입의 필드를 만들 수 있다.********

⇒ properties아래에 하위 필드 이름과 타입을 지정한다.

단점 : **object type**은 하위필드 조회할때 모든 상위 필드가 조회된다.

**Nested → properties: characters : type = nested**

### Text Analysis

텍스트 분석은 Analyzer라고 하는 도구가 수행된다.

<aside>
👉 **캐릭터 필터링(최대3개)** → **토크나이저** → **토큰필터**

</aside>

 

- 캐릭터필터나 토큰필터는 순서가 중요하다
- Pattern Replace / Mapping / HTML Strip

### _analyze API

분석된 문장을 api를 이용해서 확인할 수 있다.

필터는 []안에 배열 형태로 저장한다.

```json
GET _analyze
{
  "text": "The quick brown fox jumps over the lazy dog",
  "tokenizer": "whitespace",
  "filter": [
    "lowercase",
    "stop",
    "snowball"
  ]
}
```

- **Character Filter**
    
    전체데이터에 적용되는 일종의 전처리 도구이다.
    
    - Pattern Replace / Mapping / HTML Strip
- **HTML Strip**
    
    입력된 텍스트가 HTML인 경우 태그를 삭제하여 일반텍스트로 만들어준다.
    
- **Mapping**
    
    지정한 단어를 다른언어로 치환이 가능하다.
    
    - **+를  _plus_로 치환하는 mapping 캐릭터 필터 예제**

```json
PUT coding
{
  "settings": {
    "analysis": {
      "analyzer": {
        "coding_analyzer": {
          "char_filter": [
            "cpp_char_filter"
          ],
          "tokenizer": "whitespace",
          "filter": [ "lowercase", "stop", "snowball" ]
        }
      },
      "char_filter": {
        "cpp_char_filter": {
          "type": "mapping",
          "mappings": [ "+ => _plus_", "- => _minus_" ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "language": {
        "type": "text",
        "analyzer": "coding_analyzer"
      }
    }
  }
}
```

**+Pattern_replace**

- **Tokenizer**
    
    ```json
    GET _analyze
    {
      "text" : "abc# !@ fox emwte",
      "tokenizer": "standard" // whitespace letter
    }
    ```
    

- **Token Filter**
    
    분리된 각각의 텀들을 지정한 규칙에 따라 처리 해주는 과정을 담당한다.
    
    배열안의 순서를 고려해야한다.
    
    종류 : **lowercase, uppercase, stop, synonym, ngram**
    
    - **Lowercase, Uppercase**
        
        대소문자가 상관없이 검색이 가능하도록 변환시켜주는 필터이다.
        
        ```json
        GET _analyze
        {
          "filter" : ["lowercase"], // uppercase
          "text": "HIEVERYONE"
        }
        ```
        
    - **Stop**
        
        불용어를 제거해주는 역할을 한다. 예시에서는 in / the / days 순이다.
        
        ```json
        //필터생성
        PUT my_stop
        {
          "settings": {
            "analysis": {
              "filter": {
                "my_stop_filter": {
                  "type": "stop",
                  "stopwords": [
                    "a",
                    "at",
                    "will"
                  ]
                }
              }
            }
          }
        }
        //분석기에 필터적용함.
        GET my_stop/_analyze
        {
          "tokenizer": "whitespace",
          "filter": [
            "lowercase",
            "my_stop_filter"
          ],
          "text": [ "Around the World in Eighty Days" ]
        }
        
        ```
        
        <aside>
        👉 **실행결과** : "token": "around", "token": "world","token": "eighty"
        
        </aside>
        
        - 불용어사전을 stopword_path를 통해 불용어 사전의 경로를 지정해준다.
        
    - **Synonym**
        
        동의어를 지정하는 필터이다.
        
        “synonyms” : []  항목 안에 배열로 저장한다.
        
        ex) synonyms : [”A,B ⇒ C”] : 왼쪽의 A,B 대신 C텀을 저장한다.
        
        ```json
        PUT my_synonym
        {
          "settings": {
            "analysis": {
              "analyzer": {
                "my_syn": {
                  "tokenizer": "whitespace",
                  "filter": [
                    "lowercase",
                    "syn_aws"
                  ]
                }
              },
              "filter": {
                "syn_aws": {
                  "type": "synonym",
                  "synonyms": [
                    "amazon, aws"
                  ]
                }
              }
            }
          },
          "mappings": {
            "properties": {
              "message": {
                "type": "text",
                "analyzer": "my_syn"
              }
            }
          }
        }
        
        PUT my_synonym/_doc/1
        { "message" : "Amazon Web Service" }
        PUT my_synonym/_doc/2
        { "message" : "AWS" }
        
        GET my_synonym/_search
        {
          "query" : {
            "match" : {
              "message" : "aws"
            }
          }
        }
        
        ```
        
    
    - **ngram**
        
        검색 텀의 일부만 미리 분리해서 저장을 할 수 있는데 이렇게 단어를 나눈 부위를 ngram이라고한다. → 메뉴의 카테고리를 작성해줄때 **(자동완성기능을 구현할 때 사용한다.)**
        
        - **min_gram(default 1), max_gram(default 2)**
        - **max_ngram_diff으로 min과 max사이의 값을 조정할 수 있다.**
        
        ```json
        PUT my_ngram
        {
        	"settings" : {
        		"analysis" : {
        			"filter" : {
        				"my_ngram" : {
        					"type" : "ngram",
        					"min_gram" : 2,
        					"max_gram" : 3
        				}
        			}
        		}
        	}
        }
        
        GET my_ngram/_analyze
        {
        	"tokenizer" : "keyword",
        	"filter" : ["my_ngram"],
        	"text" : "house"
        }
        ```
        
        <aside>
        👉 **결과 : ho hou ou ous …. use**
        
        </aside>
        
    
    - **Edge Ngram**
        
        앞쪽의 ngram만을 저장하기 위해서 사용한다.
        
        ex) (min:1 max:4) **house** = h , ho, hou, hous
        
    - **Shingle**
        
        단어 단위로 구성된 묶음으로 분리 할 때 사용한다.
        
        ex) **This is my sweet home** = this is , is my, my sweet, sweet home
        

### Simple Analyzer

단어가 아닌 문자로 텍스트를 토큰으로 분리하고 문자가 아닌 문자를 제거.

### Whitespace Analyzer

공백문자를 기준으로 자르는 analyzer

### Stop Analyzer

불용어를 제거하기 위한 필터가 추가되어있음.

### Keyword Analyzer

문자열 전체를 토큰으로 취급

### Custom Analyzer

custom토크나이저, custom토큰필터등을 조합하여 만들 수 있다.

인덱스의 settings의 index : { “analysis” : 

1. **사용할 토크나이저 지정.**
2. **토크나이저에 해당하는 옵션을 지정**
3. **사용할 토큰 필터를 정의**
4. **토큰 필터에 대한 옵션을 지정**
5. **애널라이저의 명을 정하고 사용한 토큰나이저와 토큰필터를 작성**
6. **mappings의 필드에 정의해준 애널라이저의 명을 지정해준다.**

### nori(한글분석기)

애널라이저 : nori

토크나이저 : nori_tokenizer

토큰필터 : nori_part_of_speech, nori_readingform, nori_number

- **nori_tokenizer**
    
    **user_dictionary_rules, user_dictionary**
    
    “동해물과 백두산이” → “동해” “물” ….
    
    **user_dictionary_rules** : 사용자 정의사전을 배열로 입력합니다.
    
    **nori_part_of_speech** : 조사 형용사를 제거하는 것
    
    ```json
    PUT my_pos
    {
      "settings": {
        "analysis": {
          "filter": {
            "my_pos_f": {
              "type": "nori_part_of_speech",
              "stoptags": [
                "NR", "J"
              ]
            }
          }
        }
      }
    }
    ```
    
    ```json
    GET my_pos/_analyze
    {
      "tokenizer": "nori_tokenizer",
      "filter": [
        "my_pos_f"
      ],
      "text": "다섯아이가"
    }
    //수사인 다섯이랑 '가'제거된다.
    ```
    
- **nori_readingform**
    
    한자를 한글로 변환해주는 필터이다.
    
- **nori_number**
    
    한글 숫자를 숫자로 바꿔주는 필터이다.
    

- day3

[AWS / Elastic Day3](https://www.notion.so/AWS-Elastic-Day3-d634a33f9dfe4128befe69d14c68e838)